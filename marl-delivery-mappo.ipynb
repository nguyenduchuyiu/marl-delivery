{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/cuongtv312/marl-delivery.git marl_delivery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T14:40:27.888079Z",
     "iopub.status.busy": "2025-05-10T14:40:27.887783Z",
     "iopub.status.idle": "2025-05-10T14:40:31.992683Z",
     "shell.execute_reply": "2025-05-10T14:40:31.992034Z",
     "shell.execute_reply.started": "2025-05-10T14:40:27.888058Z"
    },
    "id": "309nvG-V8Otr",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from env import Environment\n",
    "except:\n",
    "    from marl_delivery.env import Environment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.calibration import LabelEncoder # For action conversion\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T14:40:31.993737Z",
     "iopub.status.busy": "2025-05-10T14:40:31.993359Z",
     "iopub.status.idle": "2025-05-10T14:40:32.002553Z",
     "shell.execute_reply": "2025-05-10T14:40:32.001743Z",
     "shell.execute_reply.started": "2025-05-10T14:40:31.993718Z"
    },
    "id": "rq1hlk4b8Q37",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert_observation(state, persistent_packages, current_robot_idx):\n",
    "    \"\"\"\n",
    "    Convert state to a 2D multi-channel tensor for a specific robot.\n",
    "    - 6 channels for robot-specific observation:\n",
    "        0. Map\n",
    "        1. Urgency of 'waiting' packages (if robot is not carrying)\n",
    "        2. Start positions of 'waiting' packages (if robot is not carrying)\n",
    "        3. Other robots' positions\n",
    "        4. Current robot's position\n",
    "        5. Current robot's carried package target (if robot is carrying)\n",
    "\n",
    "    Args:\n",
    "        state (dict): Raw state from the environment.\n",
    "                      Expected keys: \"map\", \"robots\", \"time_step\".\n",
    "                      state[\"robots\"] is a list of tuples: (pos_x+1, pos_y+1, carrying_package_id)\n",
    "        persistent_packages (dict): Dictionary tracking all active packages.\n",
    "                                    Positions are 0-indexed.\n",
    "        current_robot_idx (int): Index of the current robot for which to generate the observation.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray of shape (6, n_rows, n_cols)\n",
    "    \"\"\"\n",
    "    grid = np.array(state[\"map\"])\n",
    "    n_rows, n_cols = grid.shape\n",
    "    n_channels = 6\n",
    "    tensor = np.zeros((n_channels, n_rows, n_cols), dtype=np.float32)\n",
    "\n",
    "    # --- Channel 0: Map ---\n",
    "    tensor[0] = grid\n",
    "\n",
    "    current_time_step = state[\"time_step\"]\n",
    "    if isinstance(current_time_step, np.ndarray): # Handle case where time_step might be an array\n",
    "        current_time_step = current_time_step[0]\n",
    "\n",
    "    # Get current robot's data and determine if it's carrying a package\n",
    "    # Ensure current_robot_idx is valid\n",
    "    if current_robot_idx < 0 or current_robot_idx >= len(state[\"robots\"]):\n",
    "        # This case should ideally be handled by the caller or indicate an error\n",
    "        # print(f\"Warning: Invalid current_robot_idx {current_robot_idx}\")\n",
    "        return tensor # Return empty tensor or handle error appropriately\n",
    "\n",
    "    current_robot_data = state[\"robots\"][current_robot_idx]\n",
    "    carried_pkg_id_by_current_robot = current_robot_data[2] # 1-indexed ID, 0 if not carrying\n",
    "\n",
    "    # --- Channel 1: Urgency of 'waiting' packages (if robot is not carrying) ---\n",
    "    # --- Channel 2: Start positions of 'waiting' packages (if robot is not carrying) ---\n",
    "    if carried_pkg_id_by_current_robot == 0: # Robot is NOT carrying a package\n",
    "        for pkg_id, pkg_data in persistent_packages.items():\n",
    "            if pkg_data['status'] == 'waiting':\n",
    "                sr, sc = pkg_data['start_pos']  # 0-indexed\n",
    "                st = pkg_data['start_time']\n",
    "                dl = pkg_data['deadline']\n",
    "\n",
    "                # Check if package is active (start_time has passed)\n",
    "                if current_time_step >= st:\n",
    "                    # Channel 1: Urgency\n",
    "                    urgency = 0\n",
    "                    if dl > st: # Avoid division by zero or negative duration\n",
    "                        # Normalize urgency: 0 (just appeared) to 1 (deadline reached)\n",
    "                        # Cap at 1 if current_time_step exceeds deadline\n",
    "                        urgency = min(1.0, max(0.0, (current_time_step - st) / (dl - st)))\n",
    "                    elif dl == st: # Deadline is the start time\n",
    "                         urgency = 1.0 if current_time_step >= st else 0.0\n",
    "                    # else: dl < st, invalid, urgency remains 0\n",
    "\n",
    "                    if 0 <= sr < n_rows and 0 <= sc < n_cols: # Boundary check\n",
    "                        tensor[1, sr, sc] = max(tensor[1, sr, sc], urgency) # Use max if multiple pkgs at same spot\n",
    "\n",
    "                    # Channel 2: Start position\n",
    "                    if 0 <= sr < n_rows and 0 <= sc < n_cols: # Boundary check\n",
    "                        tensor[2, sr, sc] = 1.0 # Mark presence\n",
    "    # If robot is carrying, channels 1 and 2 remain all zeros.\n",
    "\n",
    "    # --- Channel 3: Other robots' positions ---\n",
    "    for i, rob_data in enumerate(state[\"robots\"]):\n",
    "        if i == current_robot_idx:\n",
    "            continue # Skip the current robot\n",
    "        rr, rc, _ = rob_data # Positions are 1-indexed from env\n",
    "        rr_idx, rc_idx = int(rr) - 1, int(rc) - 1 # Convert to 0-indexed\n",
    "        if 0 <= rr_idx < n_rows and 0 <= rc_idx < n_cols: # Boundary check\n",
    "            tensor[3, rr_idx, rc_idx] = 1.0\n",
    "\n",
    "    # --- Channel 4: Current robot's position ---\n",
    "    # current_robot_data was fetched earlier\n",
    "    crr, crc, _ = current_robot_data # Positions are 1-indexed\n",
    "    crr_idx, crc_idx = int(crr) - 1, int(crc) - 1 # Convert to 0-indexed\n",
    "    if 0 <= crr_idx < n_rows and 0 <= crc_idx < n_cols: # Boundary check\n",
    "        tensor[4, crr_idx, crc_idx] = 1.0\n",
    "\n",
    "    # --- Channel 5: Current robot's carried package target (if robot is carrying) ---\n",
    "    if carried_pkg_id_by_current_robot != 0:\n",
    "        # Ensure the package ID from state['robots'] is valid and exists in persistent_packages\n",
    "        if carried_pkg_id_by_current_robot in persistent_packages:\n",
    "            pkg_data_carried = persistent_packages[carried_pkg_id_by_current_robot]\n",
    "            # Double check status, though if robot carries it, it should be 'in_transit'\n",
    "            # or just became 'in_transit' in the persistent_packages update logic.\n",
    "            # For this observation, we primarily care about its target.\n",
    "            tr_carried, tc_carried = pkg_data_carried['target_pos'] # 0-indexed\n",
    "            if 0 <= tr_carried < n_rows and 0 <= tc_carried < n_cols: # Boundary check\n",
    "                tensor[5, tr_carried, tc_carried] = 1.0\n",
    "        # else:\n",
    "            # This case might indicate an inconsistency.\n",
    "            # print(f\"Warning: Robot {current_robot_idx} carrying pkg {carried_pkg_id_by_current_robot} not in persistent_packages.\")\n",
    "    # If robot is not carrying, channel 5 remains all zeros.\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_state(state_dict, persistent_packages, state_tensor_shape):\n",
    "    \"\"\"\n",
    "    Converts the global state dictionary to a tensor for QMIX.\n",
    "    Relies on `persistent_packages` for all package information.\n",
    "    The `packages` key in `state_dict` (if present) is ignored for package data.\n",
    "\n",
    "    Args:\n",
    "        state_dict (dict): The raw environment state dictionary.\n",
    "                           Expected keys: \"map\", \"robots\", \"time_step\".\n",
    "        persistent_packages (dict): Dictionary tracking all active packages.\n",
    "                                    Positions are 0-indexed.\n",
    "                                    Example entry:\n",
    "                                    { pkg_id: {'start_pos': (r,c), 'target_pos': (r,c),\n",
    "                                                'status': 'waiting'/'in_transit',\n",
    "                                                'start_time': ts, 'deadline': dl, 'id': pkg_id} }\n",
    "        state_tensor_shape (tuple): Tuple (num_channels, n_rows, n_cols) for the output state tensor.\n",
    "        max_time_steps (int): Maximum time steps in an episode for normalization.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The global state tensor with shape specified by state_tensor_shape.\n",
    "        float: Normalized current time step (scalar feature).\n",
    "    \"\"\"\n",
    "    num_channels_out, n_rows, n_cols = state_tensor_shape\n",
    "    \n",
    "    spatial_tensor = np.zeros((num_channels_out, n_rows, n_cols), dtype=np.float32)\n",
    "\n",
    "    CH_IDX_MAP_OBSTACLES = 0\n",
    "    CH_IDX_ROBOT_POSITIONS = 1\n",
    "    CH_IDX_ROBOT_CARRYING_STATUS = 2\n",
    "    CH_IDX_PKG_WAITING_START_POS = 3\n",
    "    CH_IDX_PKG_WAITING_TARGET_POS = 4\n",
    "    CH_IDX_PKG_IN_TRANSIT_TARGET_POS = 5\n",
    "    CH_IDX_PKG_WAITING_URGENCY = 6\n",
    "\n",
    "    # --- Channel: Map Obstacles (Centering/Cropping Logic) ---\n",
    "    if CH_IDX_MAP_OBSTACLES < num_channels_out:\n",
    "        game_map_from_state = np.array(state_dict[\"map\"])\n",
    "        map_rows_src, map_cols_src = game_map_from_state.shape\n",
    "\n",
    "        src_r_start = (map_rows_src - n_rows) // 2 if map_rows_src > n_rows else 0\n",
    "        src_c_start = (map_cols_src - n_cols) // 2 if map_cols_src > n_cols else 0\n",
    "        \n",
    "        rows_to_copy_from_src = min(map_rows_src, n_rows)\n",
    "        cols_to_copy_from_src = min(map_cols_src, n_cols)\n",
    "\n",
    "        map_section_to_copy = game_map_from_state[\n",
    "            src_r_start : src_r_start + rows_to_copy_from_src,\n",
    "            src_c_start : src_c_start + cols_to_copy_from_src\n",
    "        ]\n",
    "        \n",
    "        target_r_offset = (n_rows - map_section_to_copy.shape[0]) // 2\n",
    "        target_c_offset = (n_cols - map_section_to_copy.shape[1]) // 2\n",
    "            \n",
    "        spatial_tensor[\n",
    "            CH_IDX_MAP_OBSTACLES,\n",
    "            target_r_offset : target_r_offset + map_section_to_copy.shape[0],\n",
    "            target_c_offset : target_c_offset + map_section_to_copy.shape[1]\n",
    "        ] = map_section_to_copy\n",
    "\n",
    "    # --- Current Time (Scalar Feature) ---\n",
    "    current_time = state_dict[\"time_step\"]\n",
    "\n",
    "    # --- Channels: Robot Positions and Carrying Status (from state_dict['robots']) ---\n",
    "    if 'robots' in state_dict and state_dict['robots'] is not None:\n",
    "        for r_data in state_dict['robots']:\n",
    "            # r_data: (pos_r_1idx, pos_c_1idx, carrying_package_id)\n",
    "            r_idx, c_idx = int(r_data[0]) - 1, int(r_data[1]) - 1 # Convert to 0-indexed\n",
    "            carried_pkg_id = r_data[2]\n",
    "\n",
    "            if 0 <= r_idx < n_rows and 0 <= c_idx < n_cols: # Boundary check\n",
    "                if CH_IDX_ROBOT_POSITIONS < num_channels_out:\n",
    "                    spatial_tensor[CH_IDX_ROBOT_POSITIONS, r_idx, c_idx] = 1.0\n",
    "                \n",
    "                if carried_pkg_id != 0 and CH_IDX_ROBOT_CARRYING_STATUS < num_channels_out:\n",
    "                    spatial_tensor[CH_IDX_ROBOT_CARRYING_STATUS, r_idx, c_idx] = 1.0\n",
    "\n",
    "    # --- Process persistent_packages for ALL package-related channels ---\n",
    "    # Note: state_dict['packages'] is NOT used here.\n",
    "    for pkg_id, pkg_data in persistent_packages.items():\n",
    "        start_pos = pkg_data['start_pos']   # Expected (r, c) 0-indexed\n",
    "        target_pos = pkg_data['target_pos'] # Expected (r, c) 0-indexed\n",
    "        status = pkg_data['status']\n",
    "        pkg_start_time = pkg_data['start_time']\n",
    "        pkg_deadline = pkg_data['deadline']\n",
    "        \n",
    "        # Process only if package is active based on its start_time\n",
    "        if current_time >= pkg_start_time:\n",
    "            if status == 'waiting':\n",
    "                # Channel: Waiting Packages' Start Positions\n",
    "                if CH_IDX_PKG_WAITING_START_POS < num_channels_out:\n",
    "                    if 0 <= start_pos[0] < n_rows and 0 <= start_pos[1] < n_cols: # Boundary check\n",
    "                        spatial_tensor[CH_IDX_PKG_WAITING_START_POS, start_pos[0], start_pos[1]] = 1.0\n",
    "\n",
    "                # Channel: Urgency of Waiting Packages\n",
    "                if CH_IDX_PKG_WAITING_URGENCY < num_channels_out:\n",
    "                    if 0 <= start_pos[0] < n_rows and 0 <= start_pos[1] < n_cols: # Boundary check\n",
    "                        urgency = 0.0\n",
    "                        if pkg_deadline > pkg_start_time: \n",
    "                            urgency = min(1.0, max(0.0, (current_time - pkg_start_time) / (pkg_deadline - pkg_start_time)))\n",
    "                        elif pkg_deadline == pkg_start_time: \n",
    "                            urgency = 1.0 # Deadline is now or passed if current_time >= pkg_start_time\n",
    "                        # Use max in case multiple packages share the same start_pos\n",
    "                        spatial_tensor[CH_IDX_PKG_WAITING_URGENCY, start_pos[0], start_pos[1]] = \\\n",
    "                            max(spatial_tensor[CH_IDX_PKG_WAITING_URGENCY, start_pos[0], start_pos[1]], urgency)\n",
    "\n",
    "                # Channel: Waiting Packages' Target Positions\n",
    "                if CH_IDX_PKG_WAITING_TARGET_POS < num_channels_out:\n",
    "                    if 0 <= target_pos[0] < n_rows and 0 <= target_pos[1] < n_cols: # Boundary check\n",
    "                        spatial_tensor[CH_IDX_PKG_WAITING_TARGET_POS, target_pos[0], target_pos[1]] = \\\n",
    "                            max(spatial_tensor[CH_IDX_PKG_WAITING_TARGET_POS, target_pos[0], target_pos[1]], 1.0)\n",
    "            \n",
    "            elif status == 'in_transit':\n",
    "                # Channel: In-Transit Packages' Target Positions\n",
    "                if CH_IDX_PKG_IN_TRANSIT_TARGET_POS < num_channels_out:\n",
    "                    if 0 <= target_pos[0] < n_rows and 0 <= target_pos[1] < n_cols: # Boundary check\n",
    "                        spatial_tensor[CH_IDX_PKG_IN_TRANSIT_TARGET_POS, target_pos[0], target_pos[1]] = \\\n",
    "                            max(spatial_tensor[CH_IDX_PKG_IN_TRANSIT_TARGET_POS, target_pos[0], target_pos[1]], 1.0)\n",
    "                \n",
    "    return spatial_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T14:40:33.270512Z",
     "iopub.status.busy": "2025-05-10T14:40:33.269885Z",
     "iopub.status.idle": "2025-05-10T14:40:33.274301Z",
     "shell.execute_reply": "2025-05-10T14:40:33.273654Z",
     "shell.execute_reply.started": "2025-05-10T14:40:33.270464Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- MAPPO Hyperparameters ---\n",
    "ACTION_DIM = 15  # Total discrete actions for an agent\n",
    "NUM_AGENTS = 5\n",
    "MAP_FILE = \"map1.txt\"\n",
    "N_PACKAGES = 20\n",
    "MOVE_COST = -0.01 # Adjusted for PPO, rewards should be reasonably scaled\n",
    "DELIVERY_REWARD = 10\n",
    "DELAY_REWARD = 1 # Or 0, depending on reward shaping strategy\n",
    "MAX_TIME_STEPS_PER_EPISODE = 1000 # Max steps for one episode in one env\n",
    "\n",
    "NUM_ENVS = 1  # Number of parallel environments\n",
    "ROLLOUT_STEPS = 1024 # Number of steps to collect data for before an update\n",
    "TOTAL_TIMESTEPS = 1_000_000 # Total timesteps for training\n",
    "\n",
    "# PPO specific\n",
    "LR_ACTOR = 5e-5\n",
    "LR_CRITIC = 5e-5\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "CLIP_EPS = 0.2\n",
    "NUM_EPOCHS = 10 # Number of epochs to train on collected data\n",
    "MINIBATCH_SIZE = 64 # Minibatch size for PPO updates\n",
    "ENTROPY_COEF = 0.01\n",
    "VALUE_LOSS_COEF = 0.5\n",
    "MAX_GRAD_NORM = 0.5\n",
    "WEIGHT_DECAY = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Network for MAPPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, obs_shape, action_dim):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        # obs_shape is (C, H, W) e.g. (6, map_height, map_width)\n",
    "        self.conv1 = nn.Conv2d(obs_shape[0], 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Calculate flattened size after conv layers\n",
    "        def conv_out_size(h_in, w_in):\n",
    "            # Assuming kernel=3, stride=1, padding=1 keeps H, W same\n",
    "            return h_in * w_in * 64\n",
    "\n",
    "        self.flattened_size = conv_out_size(obs_shape[1], obs_shape[2])\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.flattened_size, 256)\n",
    "        self.actor_head = nn.Linear(256, action_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # obs: (batch_size, C, H, W)\n",
    "        x = F.relu(self.bn1(self.conv1(obs)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = x.reshape(x.size(0), -1) # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        action_logits = self.actor_head(x)\n",
    "        return action_logits\n",
    "\n",
    "# Critic Network for MAPPO\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, global_state_shape):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        # global_state_shape is (C_global, H, W) e.g. (7, map_height, map_width)\n",
    "        self.conv1 = nn.Conv2d(global_state_shape[0], 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        def conv_out_size(h_in, w_in):\n",
    "            return h_in * w_in * 64\n",
    "            \n",
    "        self.flattened_size = conv_out_size(global_state_shape[1], global_state_shape[2])\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 256)\n",
    "        self.critic_head = nn.Linear(256, 1) # Outputs a single value\n",
    "\n",
    "    def forward(self, global_state):\n",
    "        # global_state: (batch_size, C_global, H, W)\n",
    "        x = F.relu(self.bn1(self.conv1(global_state)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = x.reshape(x.size(0), -1) # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        value = self.critic_head(x)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mappo_model(actor, critic, path_prefix=\"models/mappo\"):\n",
    "    if not os.path.exists(os.path.dirname(path_prefix)):\n",
    "        os.makedirs(os.path.dirname(path_prefix))\n",
    "    torch.save(actor.state_dict(), f\"{path_prefix}_actor.pt\")\n",
    "    torch.save(critic.state_dict(), f\"{path_prefix}_critic.pt\")\n",
    "    print(f\"MAPPO models saved with prefix {path_prefix}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mappo_model(actor, critic, path_prefix=\"models/mappo\"):\n",
    "    actor_path = f\"{path_prefix}_actor.pt\"\n",
    "    critic_path = f\"{path_prefix}_critic.pt\"\n",
    "    if os.path.exists(actor_path) and os.path.exists(critic_path):\n",
    "        actor.load_state_dict(torch.load(actor_path, map_location=device))\n",
    "        critic.load_state_dict(torch.load(critic_path, map_location=device))\n",
    "        print(f\"MAPPO models loaded from prefix {path_prefix}\")\n",
    "        return True\n",
    "    print(f\"Could not find MAPPO models at prefix {path_prefix}\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_shaping(\n",
    "    prev_env_state,\n",
    "    current_env_state,\n",
    "    actions_taken,\n",
    "    persistent_packages_before_action,\n",
    "    num_agents\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute shaped rewards for each agent using only carrying_id transitions.\n",
    "    \"\"\"\n",
    "    # --- Constants ---\n",
    "    SHAPING_SUCCESSFUL_PICKUP_BONUS = 50\n",
    "    SHAPING_SUCCESSFUL_DELIVERY_BONUS = 1000\n",
    "    SHAPING_LATE_DELIVERY_PENALTY = -900\n",
    "    SHAPING_WRONG_DROP_LOCATION_PENALTY = -10 # never happens\n",
    "    SHAPING_WASTED_PICKUP_PENALTY = -0.01\n",
    "    SHAPING_WASTED_DROP_PENALTY = 0\n",
    "    SHAPING_FAILED_INTENDED_PICKUP_PENALTY = -0.01\n",
    "    SHAPING_FAILED_INTENDED_DROP_PENALTY = -0.01\n",
    "    SHAPING_STAY_PENALTY = -0.01\n",
    "    MOVE_COST = -0.01\n",
    "\n",
    "    individual_rewards = np.array([MOVE_COST] * num_agents)\n",
    "\n",
    "    current_time_from_env = current_env_state['time_step']\n",
    "    if isinstance(current_time_from_env, np.ndarray):\n",
    "        current_time_from_env = current_time_from_env[0]\n",
    "\n",
    "    time_at_prev_state = prev_env_state.get('time_step', current_time_from_env - 1)\n",
    "    if isinstance(time_at_prev_state, np.ndarray):\n",
    "        time_at_prev_state = time_at_prev_state[0]\n",
    "\n",
    "    for i in range(num_agents):\n",
    "        agent_action = actions_taken[i]\n",
    "        package_op = int(agent_action[1])  # 0: None, 1: Pick, 2: Drop\n",
    "\n",
    "        prev_robot_info = prev_env_state['robots'][i]\n",
    "        current_robot_info = current_env_state['robots'][i]\n",
    "\n",
    "        robot_prev_pos_0idx = (prev_robot_info[0] - 1, prev_robot_info[1] - 1)\n",
    "        robot_current_pos_0idx = (current_robot_info[0] - 1, current_robot_info[1] - 1)\n",
    "\n",
    "        prev_carrying_id = prev_robot_info[2]\n",
    "        current_carrying_id = current_robot_info[2]\n",
    "\n",
    "        # Penalty for staying in place if there are waiting packages\n",
    "        waiting_packages_exist = any(\n",
    "            pkg['status'] == 'waiting' and pkg['start_time'] <= time_at_prev_state\n",
    "            for pkg in persistent_packages_before_action.values()\n",
    "        )\n",
    "        if robot_prev_pos_0idx == robot_current_pos_0idx and waiting_packages_exist:\n",
    "            individual_rewards[i] += SHAPING_STAY_PENALTY\n",
    "            # print(f\"Agent {i}: Stay penalty. Reward: {SHAPING_STAY_PENALTY}\")\n",
    "        elif robot_prev_pos_0idx == robot_current_pos_0idx and not waiting_packages_exist:\n",
    "            individual_rewards[i] -= SHAPING_STAY_PENALTY  # reward for not moving if no waiting packages\n",
    "            # print(f\"Agent {i}: Cancel move cost. Reward: {SHAPING_STAY_PENALTY}\")\n",
    "\n",
    "        # --- Xử lý dựa trên thay đổi trạng thái mang hàng (carrying status) ---\n",
    "        if prev_carrying_id == 0 and current_carrying_id != 0:\n",
    "            # Sự kiện: Robot đã NHẶT được một gói hàng\n",
    "            individual_rewards[i] += SHAPING_SUCCESSFUL_PICKUP_BONUS\n",
    "            # print(f\"Agent {i}: Successful pickup. Reward: {SHAPING_SUCCESSFUL_PICKUP_BONUS}\")\n",
    "\n",
    "        elif prev_carrying_id != 0 and current_carrying_id == 0:\n",
    "            # Sự kiện: Robot đã THẢ một gói hàng\n",
    "            dropped_pkg_id = prev_carrying_id\n",
    "            if dropped_pkg_id in persistent_packages_before_action:\n",
    "                pkg_info = persistent_packages_before_action[dropped_pkg_id]\n",
    "                pkg_target_pos_0idx = pkg_info['target_pos']\n",
    "                pkg_deadline = pkg_info['deadline']\n",
    "\n",
    "                if robot_current_pos_0idx == pkg_target_pos_0idx:\n",
    "                    # Thả hàng ĐÚNG vị trí đích\n",
    "                    individual_rewards[i] += SHAPING_SUCCESSFUL_DELIVERY_BONUS\n",
    "                    # print(f\"Agent {i}: Successful delivery. Reward: {SHAPING_SUCCESSFUL_DELIVERY_BONUS}\")\n",
    "                    if current_time_from_env > pkg_deadline:\n",
    "                        individual_rewards[i] += SHAPING_LATE_DELIVERY_PENALTY\n",
    "                        # print(f\"Agent {i}: Late delivery penalty. Reward: {SHAPING_LATE_DELIVERY_PENALTY}\")\n",
    "                else:\n",
    "                    # Thả hàng SAI vị trí đích\n",
    "                    individual_rewards[i] += SHAPING_WRONG_DROP_LOCATION_PENALTY\n",
    "                    # print(f\"Agent {i}: Wrong drop location. Reward: {SHAPING_WRONG_DROP_LOCATION_PENALTY}\")\n",
    "            # else: dropped a package not in persistent_packages_before_action (should not happen)\n",
    "\n",
    "        # --- Xử lý các ý định (package_op) không dẫn đến thay đổi trạng thái mong muốn hoặc không hợp lệ ---\n",
    "        else:\n",
    "            if package_op == 1:  # Robot CỐ GẮNG nhặt hàng\n",
    "                if prev_carrying_id != 0:\n",
    "                    # Cố nhặt khi đang mang hàng\n",
    "                    individual_rewards[i] += SHAPING_WASTED_PICKUP_PENALTY\n",
    "                    # print(f\"Agent {i}: Wasted pickup attempt (already carrying). Reward: {SHAPING_WASTED_PICKUP_PENALTY}\")\n",
    "                elif prev_carrying_id == 0 and current_carrying_id == 0:\n",
    "                    # Cố nhặt khi không mang gì, và sau đó vẫn không mang gì (nhặt thất bại)\n",
    "                    individual_rewards[i] += SHAPING_FAILED_INTENDED_PICKUP_PENALTY\n",
    "                    # print(f\"Agent {i}: Failed intended pickup. Reward: {SHAPING_FAILED_INTENDED_PICKUP_PENALTY}\")\n",
    "\n",
    "            elif package_op == 2:  # Robot CỐ GẮNG thả hàng\n",
    "                if prev_carrying_id == 0:\n",
    "                    # Cố thả khi không mang gì\n",
    "                    individual_rewards[i] += SHAPING_WASTED_DROP_PENALTY\n",
    "                    # print(f\"Agent {i}: Wasted drop attempt (not carrying). Reward: {SHAPING_WASTED_DROP_PENALTY}\")\n",
    "                elif prev_carrying_id != 0 and current_carrying_id != 0:\n",
    "                    # Cố thả khi đang mang hàng, và sau đó vẫn đang mang hàng (thả thất bại)\n",
    "                    individual_rewards[i] += SHAPING_FAILED_INTENDED_DROP_PENALTY\n",
    "                    # print(f\"Agent {i}: Failed intended drop. Reward: {SHAPING_FAILED_INTENDED_DROP_PENALTY}\")\n",
    "    \n",
    "    # print(f\"Total rewards: {individual_rewards.sum()}\")\n",
    "    return individual_rewards.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizedEnv:\n",
    "    def __init__(self, env_cls, num_envs, **env_kwargs):\n",
    "        # Assign a unique seed to each environment if 'seed' is in env_kwargs\n",
    "        base_seed = env_kwargs.get('seed', None)\n",
    "        self.envs = []\n",
    "        for idx in range(num_envs):\n",
    "            env_args = env_kwargs.copy()\n",
    "            if base_seed is not None:\n",
    "                env_args['seed'] = base_seed + idx\n",
    "            self.envs.append(env_cls(**env_args))\n",
    "        self.num_envs = num_envs\n",
    "\n",
    "    def reset(self):\n",
    "        return [env.reset() for env in self.envs]\n",
    "\n",
    "    def step(self, actions):\n",
    "        # actions: list of actions for each env\n",
    "        results = [env.step(action) for env, action in zip(self.envs, actions)]\n",
    "        next_states, rewards, dones, infos = zip(*results)\n",
    "        return list(next_states), list(rewards), list(dones), list(infos)\n",
    "\n",
    "    def render(self):\n",
    "        for env in self.envs:\n",
    "            env.render_pygame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T14:40:34.520622Z",
     "iopub.status.busy": "2025-05-10T14:40:34.520368Z",
     "iopub.status.idle": "2025-05-10T14:40:34.540082Z",
     "shell.execute_reply": "2025-05-10T14:40:34.539365Z",
     "shell.execute_reply.started": "2025-05-10T14:40:34.520607Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MAPPOTrainer:\n",
    "    def __init__(self, vec_env, num_agents, action_dim, obs_shape, global_state_shape):\n",
    "        self.vec_env = vec_env\n",
    "        self.num_envs = vec_env.num_envs\n",
    "        self.num_agents = num_agents\n",
    "        self.action_dim = action_dim\n",
    "        self.obs_shape = obs_shape # (C, H, W) for local obs\n",
    "        self.global_state_shape = global_state_shape # (C_global, H, W) for global state\n",
    "\n",
    "        self.actor = ActorNetwork(obs_shape, action_dim).to(device)\n",
    "        self.critic = CriticNetwork(global_state_shape).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=LR_ACTOR, weight_decay=WEIGHT_DECAY)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # For converting integer actions to environment actions\n",
    "        self.le_move = LabelEncoder()\n",
    "        self.le_move.fit(['S', 'L', 'R', 'U', 'D'])\n",
    "        self.le_pkg_op = LabelEncoder()\n",
    "        self.le_pkg_op.fit(['0', '1', '2']) # 0: None, 1: Pickup, 2: Drop\n",
    "        self.NUM_MOVE_ACTIONS = len(self.le_move.classes_)\n",
    "        self.NUM_PKG_OPS = len(self.le_pkg_op.classes_)\n",
    "        \n",
    "        # Persistent packages trackers for each environment (for state conversion)\n",
    "        self.persistent_packages_list = [{} for _ in range(self.num_envs)]\n",
    "\n",
    "\n",
    "    def _update_persistent_packages_for_env(self, env_idx, current_env_state_dict):\n",
    "        # This is a simplified version of the DQNTrainer's method, adapted for one env\n",
    "        current_persistent_packages = self.persistent_packages_list[env_idx]\n",
    "        \n",
    "        if 'packages' in current_env_state_dict and current_env_state_dict['packages'] is not None:\n",
    "            for pkg_tuple in current_env_state_dict['packages']:\n",
    "                pkg_id = pkg_tuple[0]\n",
    "                if pkg_id not in current_persistent_packages:\n",
    "                    current_persistent_packages[pkg_id] = {\n",
    "                        'id': pkg_id,\n",
    "                        'start_pos': (pkg_tuple[1] - 1, pkg_tuple[2] - 1),\n",
    "                        'target_pos': (pkg_tuple[3] - 1, pkg_tuple[4] - 1),\n",
    "                        'start_time': pkg_tuple[5],\n",
    "                        'deadline': pkg_tuple[6],\n",
    "                        'status': 'waiting'\n",
    "                    }\n",
    "\n",
    "        current_carried_pkg_ids_set = set()\n",
    "        if 'robots' in current_env_state_dict and current_env_state_dict['robots'] is not None:\n",
    "            for r_data in current_env_state_dict['robots']:\n",
    "                carried_id = r_data[2]\n",
    "                if carried_id != 0:\n",
    "                    current_carried_pkg_ids_set.add(carried_id)\n",
    "\n",
    "        packages_to_remove = []\n",
    "        for pkg_id, pkg_data in list(current_persistent_packages.items()):\n",
    "            if pkg_id in current_carried_pkg_ids_set:\n",
    "                current_persistent_packages[pkg_id]['status'] = 'in_transit'\n",
    "            else:\n",
    "                if pkg_data['status'] == 'in_transit':\n",
    "                    packages_to_remove.append(pkg_id)\n",
    "        \n",
    "        for pkg_id_to_remove in packages_to_remove:\n",
    "            if pkg_id_to_remove in current_persistent_packages:\n",
    "                del current_persistent_packages[pkg_id_to_remove]\n",
    "        self.persistent_packages_list[env_idx] = current_persistent_packages\n",
    "\n",
    "\n",
    "    def _get_actions_and_values(self, current_local_obs_b_a_c_h_w, current_global_states_b_c_h_w):\n",
    "        # Ensure input tensors are on the correct device\n",
    "        current_local_obs_b_a_c_h_w = current_local_obs_b_a_c_h_w.to(device)\n",
    "        current_global_states_b_c_h_w = current_global_states_b_c_h_w.to(device)\n",
    "        \n",
    "        actor_input_obs = current_local_obs_b_a_c_h_w.reshape(self.num_envs * self.num_agents, self.obs_shape[0], self.obs_shape[1], self.obs_shape[2])\n",
    "        action_logits = self.actor(actor_input_obs) # (NUM_ENVS * NUM_AGENTS, ACTION_DIM)\n",
    "        dist = Categorical(logits=action_logits)\n",
    "        actions_int = dist.sample() # (NUM_ENVS * NUM_AGENTS)\n",
    "        log_probs = dist.log_prob(actions_int) # (NUM_ENVS * NUM_AGENTS)\n",
    "\n",
    "        actions_int_reshaped = actions_int.reshape(self.num_envs, self.num_agents)\n",
    "        log_probs_reshaped = log_probs.reshape(self.num_envs, self.num_agents)\n",
    "\n",
    "        values = self.critic(current_global_states_b_c_h_w) # (NUM_ENVS, 1)\n",
    "\n",
    "        return actions_int_reshaped, log_probs_reshaped, values.squeeze(-1) # values squeezed to (NUM_ENVS)\n",
    "\n",
    "    def collect_rollouts(self, current_env_states_list, current_local_obs_list, current_global_states_list):\n",
    "        # Buffers to store trajectory data\n",
    "        mb_obs = torch.zeros((ROLLOUT_STEPS, self.num_envs, self.num_agents, *self.obs_shape), device=device)\n",
    "        mb_global_states = torch.zeros((ROLLOUT_STEPS, self.num_envs, *self.global_state_shape), device=device)\n",
    "        mb_actions = torch.zeros((ROLLOUT_STEPS, self.num_envs, self.num_agents), dtype=torch.long, device=device)\n",
    "        mb_log_probs = torch.zeros((ROLLOUT_STEPS, self.num_envs, self.num_agents), device=device)\n",
    "        mb_rewards = torch.zeros((ROLLOUT_STEPS, self.num_envs), device=device)\n",
    "        mb_dones = torch.zeros((ROLLOUT_STEPS, self.num_envs), dtype=torch.bool, device=device)\n",
    "        mb_values = torch.zeros((ROLLOUT_STEPS, self.num_envs), device=device)\n",
    "\n",
    "        # Move initial obs/states to device\n",
    "        current_local_obs_list = current_local_obs_list.to(device)\n",
    "        current_global_states_list = current_global_states_list.to(device)\n",
    "\n",
    "        for step in range(ROLLOUT_STEPS):\n",
    "            # Render the environment\n",
    "            self.vec_env.render()\n",
    "            \n",
    "            mb_obs[step] = current_local_obs_list\n",
    "            mb_global_states[step] = current_global_states_list\n",
    "\n",
    "            with torch.no_grad():\n",
    "                actions_int_ne_na, log_probs_ne_na, values_ne = self._get_actions_and_values(\n",
    "                    current_local_obs_list, \n",
    "                    current_global_states_list\n",
    "                )\n",
    "            \n",
    "            mb_actions[step] = actions_int_ne_na\n",
    "            mb_log_probs[step] = log_probs_ne_na\n",
    "            mb_values[step] = values_ne\n",
    "\n",
    "            # Convert integer actions to environment compatible actions\n",
    "            env_actions_batch = []\n",
    "            for env_idx in range(self.num_envs):\n",
    "                env_agent_actions = []\n",
    "                for agent_idx in range(self.num_agents):\n",
    "                    int_act = actions_int_ne_na[env_idx, agent_idx].item()\n",
    "                    move_idx = int_act % self.NUM_MOVE_ACTIONS\n",
    "                    pkg_op_idx = int_act // self.NUM_MOVE_ACTIONS\n",
    "                    if pkg_op_idx >= self.NUM_PKG_OPS: pkg_op_idx = 0 # Safety clamp\n",
    "                    \n",
    "                    move_str = self.le_move.inverse_transform([move_idx])[0]\n",
    "                    pkg_op_str = self.le_pkg_op.inverse_transform([pkg_op_idx])[0]\n",
    "                    env_agent_actions.append((move_str, pkg_op_str))\n",
    "                env_actions_batch.append(env_agent_actions)\n",
    "\n",
    "            next_env_states_list, global_rewards_ne, dones_ne, _ = self.vec_env.step(env_actions_batch)\n",
    "            \n",
    "            # use reward shaping here\n",
    "            reshaped_global_rewards_ne = [reward_shaping(current_env_states_list[env_idx], \n",
    "                                                        next_env_states_list[env_idx], \n",
    "                                                        env_actions_batch[env_idx], \n",
    "                                                        self.persistent_packages_list[env_idx],\n",
    "                                                        self.num_agents) for env_idx in range(self.num_envs)]\n",
    "            \n",
    "            mb_rewards[step] = torch.tensor(reshaped_global_rewards_ne, dtype=torch.float32, device=device)\n",
    "            mb_dones[step] = torch.tensor(dones_ne, dtype=torch.bool, device=device)\n",
    "\n",
    "            # Prepare next observations and states\n",
    "            next_local_obs_list = torch.zeros_like(current_local_obs_list, device=device)\n",
    "            next_global_states_list = torch.zeros_like(current_global_states_list, device=device)\n",
    "\n",
    "            for env_idx in range(self.num_envs):\n",
    "                if dones_ne[env_idx]:\n",
    "                    # --- Reset environment if done ---\n",
    "                    reset_state = self.vec_env.envs[env_idx].reset()\n",
    "                    self._update_persistent_packages_for_env(env_idx, reset_state)\n",
    "                    current_persistent_packages = self.persistent_packages_list[env_idx]\n",
    "                    next_env_states_list[env_idx] = reset_state\n",
    "                    # Update global state and local obs after reset\n",
    "                    next_global_states_list[env_idx] = torch.from_numpy(convert_state(\n",
    "                        reset_state, \n",
    "                        current_persistent_packages, \n",
    "                        self.global_state_shape\n",
    "                    )).to(device)\n",
    "                    for agent_idx in range(self.num_agents):\n",
    "                        next_local_obs_list[env_idx, agent_idx] = torch.from_numpy(\n",
    "                            convert_observation(reset_state, current_persistent_packages, agent_idx)\n",
    "                        ).float().to(device)\n",
    "                else:\n",
    "                    self._update_persistent_packages_for_env(env_idx, next_env_states_list[env_idx])\n",
    "                    current_persistent_packages = self.persistent_packages_list[env_idx]\n",
    "                    next_global_states_list[env_idx] = torch.from_numpy(convert_state(\n",
    "                        next_env_states_list[env_idx], \n",
    "                        current_persistent_packages, \n",
    "                        self.global_state_shape\n",
    "                    )).to(device)\n",
    "                    for agent_idx in range(self.num_agents):\n",
    "                        next_local_obs_list[env_idx, agent_idx] = torch.from_numpy(\n",
    "                            convert_observation(next_env_states_list[env_idx], current_persistent_packages, agent_idx)\n",
    "                        ).float().to(device)\n",
    "            \n",
    "            current_env_states_list = next_env_states_list\n",
    "            current_local_obs_list = next_local_obs_list\n",
    "            current_global_states_list = next_global_states_list\n",
    "        \n",
    "        # Calculate advantages using GAE\n",
    "        advantages = torch.zeros_like(mb_rewards, device=device)\n",
    "        last_gae_lambda = 0\n",
    "        with torch.no_grad():\n",
    "            # Get value of the last state in the rollout\n",
    "            next_value_ne = self.critic(current_global_states_list).squeeze(-1) # (NUM_ENVS)\n",
    "\n",
    "        for t in reversed(range(ROLLOUT_STEPS)):\n",
    "            next_non_terminal = 1.0 - mb_dones[t].float()\n",
    "            next_values_step = next_value_ne if t == ROLLOUT_STEPS - 1 else mb_values[t+1]\n",
    "            \n",
    "            delta = mb_rewards[t] + GAMMA * next_values_step * next_non_terminal - mb_values[t]\n",
    "            advantages[t] = last_gae_lambda = delta + GAMMA * GAE_LAMBDA * next_non_terminal * last_gae_lambda\n",
    "        \n",
    "        returns = advantages + mb_values\n",
    "\n",
    "        # Flatten the batch for training\n",
    "        b_obs = mb_obs.reshape(-1, *self.obs_shape)\n",
    "        b_global_states = mb_global_states.reshape(ROLLOUT_STEPS * self.num_envs, *self.global_state_shape)\n",
    "        b_actions = mb_actions.reshape(-1)\n",
    "        b_log_probs = mb_log_probs.reshape(-1)\n",
    "        \n",
    "        b_advantages = advantages.reshape(ROLLOUT_STEPS * self.num_envs, 1).repeat(1, self.num_agents).reshape(-1)\n",
    "        b_returns_critic = returns.reshape(-1)\n",
    "\n",
    "        return (b_obs, b_global_states, b_actions, \n",
    "           b_log_probs, b_advantages, b_returns_critic,\n",
    "           current_env_states_list, current_local_obs_list, current_global_states_list,\n",
    "           mb_rewards)\n",
    "\n",
    "    def update_ppo(self, b_obs, b_global_states, b_actions, b_log_probs_old, b_advantages, b_returns_critic):\n",
    "        # Ensure all tensors are on the correct device\n",
    "        b_obs = b_obs.to(device)\n",
    "        b_global_states = b_global_states.to(device)\n",
    "        b_actions = b_actions.to(device)\n",
    "        b_log_probs_old = b_log_probs_old.to(device)\n",
    "        b_advantages = b_advantages.to(device)\n",
    "        b_returns_critic = b_returns_critic.to(device)\n",
    "\n",
    "        num_samples_actor = ROLLOUT_STEPS * self.num_envs * self.num_agents\n",
    "        num_samples_critic = ROLLOUT_STEPS * self.num_envs\n",
    "        \n",
    "        actor_batch_indices = np.arange(num_samples_actor)\n",
    "        critic_batch_indices = np.arange(num_samples_critic)\n",
    "\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            np.random.shuffle(actor_batch_indices)\n",
    "            np.random.shuffle(critic_batch_indices)\n",
    "\n",
    "            # Actor update\n",
    "            for start in range(0, num_samples_actor, MINIBATCH_SIZE):\n",
    "                end = start + MINIBATCH_SIZE\n",
    "                mb_indices = actor_batch_indices[start:end]\n",
    "\n",
    "                mb_obs_slice = b_obs[mb_indices]\n",
    "                mb_actions_slice = b_actions[mb_indices]\n",
    "                mb_log_probs_old_slice = b_log_probs_old[mb_indices]\n",
    "                mb_advantages_slice = b_advantages[mb_indices]\n",
    "                \n",
    "                # Normalize advantages (optional but often helpful)\n",
    "                mb_advantages_slice = (mb_advantages_slice - mb_advantages_slice.mean()) / (mb_advantages_slice.std() + 1e-8)\n",
    "\n",
    "                action_logits = self.actor(mb_obs_slice)\n",
    "                dist = Categorical(logits=action_logits)\n",
    "                new_log_probs = dist.log_prob(mb_actions_slice)\n",
    "                entropy = dist.entropy().mean()\n",
    "\n",
    "                log_ratio = new_log_probs - mb_log_probs_old_slice\n",
    "                ratio = torch.exp(log_ratio)\n",
    "\n",
    "                pg_loss1 = -mb_advantages_slice * ratio\n",
    "                pg_loss2 = -mb_advantages_slice * torch.clamp(ratio, 1 - CLIP_EPS, 1 + CLIP_EPS)\n",
    "                actor_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                total_actor_loss = actor_loss - ENTROPY_COEF * entropy\n",
    "                \n",
    "                self.actor_optimizer.zero_grad()\n",
    "                total_actor_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.actor.parameters(), MAX_GRAD_NORM)\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "            # Critic update\n",
    "            for start in range(0, num_samples_critic, MINIBATCH_SIZE // self.num_agents if self.num_agents > 0 else MINIBATCH_SIZE):\n",
    "                end = start + (MINIBATCH_SIZE // self.num_agents if self.num_agents > 0 else MINIBATCH_SIZE)\n",
    "                mb_indices = critic_batch_indices[start:end]\n",
    "                \n",
    "                mb_global_states_slice = b_global_states[mb_indices]\n",
    "                mb_returns_critic_slice = b_returns_critic[mb_indices]\n",
    "\n",
    "                new_values = self.critic(mb_global_states_slice).squeeze(-1)\n",
    "                critic_loss = F.mse_loss(new_values, mb_returns_critic_slice)\n",
    "                \n",
    "                total_critic_loss = VALUE_LOSS_COEF * critic_loss\n",
    "\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                total_critic_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.critic.parameters(), MAX_GRAD_NORM)\n",
    "                self.critic_optimizer.step()\n",
    "        \n",
    "        return actor_loss.item(), critic_loss.item(), entropy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-10T17:21:49.664Z",
     "iopub.execute_input": "2025-05-10T14:40:34.541027Z",
     "iopub.status.busy": "2025-05-10T14:40:34.540841Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Obs shape: (6, 10, 10), Global state shape: (7, 10, 10)\n",
      "Starting MAPPO training...\n",
      "Mean reward this rollout: 1.0246\n",
      "Mean reward this rollout: 1.2832\n",
      "Mean reward this rollout: 2.6415\n",
      "Mean reward this rollout: 1.2137\n",
      "Mean reward this rollout: 1.3955\n",
      "Mean reward this rollout: 1.9008\n",
      "Mean reward this rollout: 1.8547\n",
      "Mean reward this rollout: 2.5677\n",
      "Mean reward this rollout: 1.9353\n",
      "Mean reward this rollout: 1.8513\n",
      "Update 10/976 | Timesteps: 10240/1000000\n",
      "  Actor Loss: -0.1250 | Critic Loss: 163.0067 | Entropy: 2.5576\n",
      "Mean reward this rollout: 0.8160\n",
      "Mean reward this rollout: 1.3811\n",
      "Mean reward this rollout: 1.2000\n",
      "Mean reward this rollout: 1.8940\n",
      "Mean reward this rollout: 1.6370\n",
      "Mean reward this rollout: 0.8175\n",
      "Mean reward this rollout: 1.4640\n",
      "Mean reward this rollout: 2.2389\n",
      "Mean reward this rollout: 1.2959\n",
      "Mean reward this rollout: 1.3548\n",
      "Update 20/976 | Timesteps: 20480/1000000\n",
      "  Actor Loss: -0.1439 | Critic Loss: 432.3106 | Entropy: 2.5523\n",
      "Mean reward this rollout: 0.9248\n",
      "Mean reward this rollout: 1.5875\n",
      "Mean reward this rollout: 0.9744\n",
      "Mean reward this rollout: 1.1700\n",
      "Mean reward this rollout: 1.1173\n",
      "Mean reward this rollout: 1.0809\n",
      "Mean reward this rollout: 1.4610\n",
      "Mean reward this rollout: 0.9752\n",
      "Mean reward this rollout: 2.1575\n",
      "Mean reward this rollout: 2.3914\n",
      "Update 30/976 | Timesteps: 30720/1000000\n",
      "  Actor Loss: -0.1011 | Critic Loss: 209.3145 | Entropy: 2.5551\n",
      "Mean reward this rollout: 1.3837\n",
      "Mean reward this rollout: 1.6062\n",
      "Mean reward this rollout: 1.3527\n",
      "Mean reward this rollout: 2.9056\n",
      "Mean reward this rollout: 1.1566\n",
      "Mean reward this rollout: 0.9907\n",
      "Mean reward this rollout: 0.6153\n",
      "Mean reward this rollout: 1.3602\n",
      "Mean reward this rollout: 1.0024\n",
      "Mean reward this rollout: 0.9013\n",
      "Update 40/976 | Timesteps: 40960/1000000\n",
      "  Actor Loss: -0.1219 | Critic Loss: 128.9867 | Entropy: 2.5348\n",
      "Mean reward this rollout: 0.2666\n",
      "Mean reward this rollout: 0.3101\n",
      "Mean reward this rollout: 0.8806\n",
      "Mean reward this rollout: 0.5703\n",
      "Mean reward this rollout: 1.5330\n",
      "Mean reward this rollout: 1.2164\n",
      "Mean reward this rollout: 1.8426\n",
      "Mean reward this rollout: 1.3050\n",
      "Mean reward this rollout: 1.7284\n",
      "Mean reward this rollout: 1.0228\n",
      "Update 50/976 | Timesteps: 51200/1000000\n",
      "  Actor Loss: -0.1308 | Critic Loss: 63.4970 | Entropy: 2.5863\n",
      "Mean reward this rollout: 1.0978\n",
      "Mean reward this rollout: 1.4917\n",
      "Mean reward this rollout: 1.1195\n",
      "Mean reward this rollout: 1.3370\n",
      "Mean reward this rollout: 1.9298\n",
      "Mean reward this rollout: 1.5431\n",
      "Mean reward this rollout: 2.1011\n",
      "Mean reward this rollout: 2.2224\n",
      "Mean reward this rollout: 0.9606\n",
      "Mean reward this rollout: 1.6461\n",
      "Update 60/976 | Timesteps: 61440/1000000\n",
      "  Actor Loss: -0.1216 | Critic Loss: 155.7120 | Entropy: 2.5713\n",
      "Mean reward this rollout: 1.4800\n",
      "Mean reward this rollout: 1.3810\n",
      "Mean reward this rollout: 2.1824\n",
      "Mean reward this rollout: 1.1643\n",
      "Mean reward this rollout: 1.3345\n",
      "Mean reward this rollout: 1.0099\n",
      "Mean reward this rollout: 3.8652\n",
      "Mean reward this rollout: 1.5004\n",
      "Mean reward this rollout: 1.3529\n",
      "Mean reward this rollout: 1.4785\n",
      "Update 70/976 | Timesteps: 71680/1000000\n",
      "  Actor Loss: -0.1330 | Critic Loss: 39.5440 | Entropy: 2.5635\n",
      "Mean reward this rollout: 2.1245\n",
      "Mean reward this rollout: 1.4935\n",
      "Mean reward this rollout: 2.1912\n",
      "Mean reward this rollout: 1.2929\n",
      "Mean reward this rollout: 0.6153\n",
      "Mean reward this rollout: 1.3901\n",
      "Mean reward this rollout: 1.0565\n",
      "Mean reward this rollout: 1.7993\n",
      "Mean reward this rollout: 2.2872\n",
      "Mean reward this rollout: 1.8530\n",
      "Update 80/976 | Timesteps: 81920/1000000\n",
      "  Actor Loss: -0.1276 | Critic Loss: 522.4012 | Entropy: 2.5722\n",
      "Mean reward this rollout: 1.7624\n",
      "Mean reward this rollout: 2.0354\n",
      "Mean reward this rollout: 1.5392\n",
      "Mean reward this rollout: 0.7725\n",
      "Mean reward this rollout: 1.9922\n",
      "Mean reward this rollout: 1.4650\n",
      "Mean reward this rollout: 1.2192\n",
      "Mean reward this rollout: 1.6485\n",
      "Mean reward this rollout: 1.3359\n",
      "Mean reward this rollout: 2.2727\n",
      "Update 90/976 | Timesteps: 92160/1000000\n",
      "  Actor Loss: -0.1363 | Critic Loss: 30.6234 | Entropy: 2.5070\n",
      "Mean reward this rollout: 1.4206\n",
      "Mean reward this rollout: 1.3127\n",
      "Mean reward this rollout: 1.5321\n",
      "Mean reward this rollout: 0.8721\n",
      "Mean reward this rollout: 1.7965\n",
      "Mean reward this rollout: 1.4020\n",
      "Mean reward this rollout: 1.0486\n",
      "Mean reward this rollout: 2.4690\n",
      "Mean reward this rollout: 1.5395\n",
      "Mean reward this rollout: 1.4998\n",
      "Update 100/976 | Timesteps: 102400/1000000\n",
      "  Actor Loss: -0.1152 | Critic Loss: 480.5452 | Entropy: 2.5066\n",
      "Saving checkpoint at update 100...\n",
      "MAPPO models saved with prefix models/mappo_update100\n",
      "Mean reward this rollout: 0.9133\n",
      "Mean reward this rollout: 1.1577\n",
      "Mean reward this rollout: 1.8990\n",
      "Mean reward this rollout: 1.2464\n",
      "Mean reward this rollout: 1.0222\n",
      "Mean reward this rollout: 1.0520\n",
      "Mean reward this rollout: 0.2801\n",
      "Mean reward this rollout: 1.0887\n",
      "Mean reward this rollout: 0.7353\n",
      "Mean reward this rollout: 1.4581\n",
      "Update 110/976 | Timesteps: 112640/1000000\n",
      "  Actor Loss: -0.1004 | Critic Loss: 130.8760 | Entropy: 2.3789\n",
      "Mean reward this rollout: 1.0579\n",
      "Mean reward this rollout: 1.4959\n",
      "Mean reward this rollout: 0.3826\n",
      "Mean reward this rollout: 0.4505\n",
      "Mean reward this rollout: 1.5422\n",
      "Mean reward this rollout: 1.4486\n",
      "Mean reward this rollout: 0.7432\n",
      "Mean reward this rollout: 1.0550\n",
      "Mean reward this rollout: 1.1076\n",
      "Mean reward this rollout: 0.7719\n",
      "Update 120/976 | Timesteps: 122880/1000000\n",
      "  Actor Loss: -0.1119 | Critic Loss: 2.4038 | Entropy: 2.4084\n",
      "Mean reward this rollout: 0.9035\n",
      "Mean reward this rollout: 1.8224\n",
      "Mean reward this rollout: 0.6690\n",
      "Mean reward this rollout: 0.8900\n",
      "Mean reward this rollout: 1.3546\n",
      "Mean reward this rollout: 0.8758\n",
      "Mean reward this rollout: 0.7648\n",
      "Mean reward this rollout: 0.7369\n",
      "Mean reward this rollout: 0.8433\n",
      "Mean reward this rollout: 1.1290\n",
      "Update 130/976 | Timesteps: 133120/1000000\n",
      "  Actor Loss: -0.1145 | Critic Loss: 163.6101 | Entropy: 2.3316\n",
      "Mean reward this rollout: 1.3191\n",
      "Mean reward this rollout: 1.0126\n",
      "Mean reward this rollout: 1.1671\n",
      "Mean reward this rollout: 1.1550\n",
      "Mean reward this rollout: 1.8481\n",
      "Mean reward this rollout: 1.3831\n",
      "Mean reward this rollout: 1.7473\n",
      "Mean reward this rollout: 1.1645\n",
      "Mean reward this rollout: 0.6664\n",
      "Mean reward this rollout: 1.0574\n",
      "Update 140/976 | Timesteps: 143360/1000000\n",
      "  Actor Loss: -0.1252 | Critic Loss: 1083.6467 | Entropy: 2.2849\n",
      "Mean reward this rollout: 1.6758\n",
      "Mean reward this rollout: 3.0072\n",
      "Mean reward this rollout: 0.9581\n",
      "Mean reward this rollout: 0.7870\n",
      "Mean reward this rollout: 1.0380\n",
      "Mean reward this rollout: 0.6592\n",
      "Mean reward this rollout: 1.5258\n",
      "Mean reward this rollout: 1.1855\n",
      "Mean reward this rollout: 0.7592\n",
      "Mean reward this rollout: 2.0157\n",
      "Update 150/976 | Timesteps: 153600/1000000\n",
      "  Actor Loss: -0.0927 | Critic Loss: 294.3311 | Entropy: 2.3600\n",
      "Mean reward this rollout: 0.8992\n",
      "Mean reward this rollout: 1.4391\n",
      "Mean reward this rollout: 1.3948\n",
      "Mean reward this rollout: 1.3138\n",
      "Mean reward this rollout: 0.8430\n",
      "Mean reward this rollout: 0.6491\n",
      "Mean reward this rollout: 1.6862\n",
      "Mean reward this rollout: 1.1101\n",
      "Mean reward this rollout: 1.4890\n",
      "Mean reward this rollout: 1.1552\n",
      "Update 160/976 | Timesteps: 163840/1000000\n",
      "  Actor Loss: -0.0779 | Critic Loss: 164.3534 | Entropy: 2.1538\n",
      "Mean reward this rollout: 0.9503\n",
      "Mean reward this rollout: 0.6064\n",
      "Mean reward this rollout: 1.1574\n",
      "Mean reward this rollout: 1.2964\n",
      "Mean reward this rollout: 0.7589\n",
      "Mean reward this rollout: 0.4579\n",
      "Mean reward this rollout: 0.9857\n",
      "Mean reward this rollout: 0.9009\n",
      "Mean reward this rollout: 0.5226\n",
      "Mean reward this rollout: 0.7167\n",
      "Update 170/976 | Timesteps: 174080/1000000\n",
      "  Actor Loss: -0.1053 | Critic Loss: 42.5657 | Entropy: 2.2290\n",
      "Mean reward this rollout: 1.2274\n",
      "Mean reward this rollout: 0.5794\n",
      "Mean reward this rollout: 0.7078\n",
      "Mean reward this rollout: 0.8928\n",
      "Mean reward this rollout: 1.1061\n",
      "Mean reward this rollout: 0.6888\n",
      "Mean reward this rollout: 1.3316\n",
      "Mean reward this rollout: 1.3622\n",
      "Mean reward this rollout: 0.5713\n",
      "Mean reward this rollout: 1.1373\n",
      "Update 180/976 | Timesteps: 184320/1000000\n",
      "  Actor Loss: -0.0619 | Critic Loss: 91.0898 | Entropy: 2.2662\n",
      "Mean reward this rollout: 0.9738\n",
      "Mean reward this rollout: 0.6251\n",
      "Mean reward this rollout: 1.4337\n",
      "Mean reward this rollout: 1.5903\n",
      "Mean reward this rollout: 1.2697\n",
      "Mean reward this rollout: 1.1911\n",
      "Mean reward this rollout: 3.0664\n",
      "Mean reward this rollout: 0.9090\n",
      "Mean reward this rollout: 2.3255\n",
      "Mean reward this rollout: 1.3082\n",
      "Update 190/976 | Timesteps: 194560/1000000\n",
      "  Actor Loss: -0.0905 | Critic Loss: 231.3820 | Entropy: 2.3669\n",
      "Mean reward this rollout: 1.4062\n",
      "Mean reward this rollout: 1.3685\n",
      "Mean reward this rollout: 0.8265\n",
      "Mean reward this rollout: 0.2089\n",
      "Mean reward this rollout: 0.4136\n",
      "Mean reward this rollout: 1.8021\n",
      "Mean reward this rollout: 1.0942\n",
      "Mean reward this rollout: 1.3975\n",
      "Mean reward this rollout: 0.8027\n",
      "Mean reward this rollout: 0.5629\n",
      "Update 200/976 | Timesteps: 204800/1000000\n",
      "  Actor Loss: -0.0593 | Critic Loss: 201.6132 | Entropy: 2.0042\n",
      "Saving checkpoint at update 200...\n",
      "MAPPO models saved with prefix models/mappo_update200\n",
      "Mean reward this rollout: 0.5476\n",
      "Mean reward this rollout: 0.5992\n",
      "Mean reward this rollout: 0.6365\n",
      "Mean reward this rollout: 0.4719\n",
      "Mean reward this rollout: 0.5181\n",
      "Mean reward this rollout: 1.7280\n",
      "Mean reward this rollout: 0.8121\n",
      "Mean reward this rollout: 0.3248\n",
      "Mean reward this rollout: 2.0471\n",
      "Mean reward this rollout: 0.3712\n",
      "Update 210/976 | Timesteps: 215040/1000000\n",
      "  Actor Loss: -0.0445 | Critic Loss: 36.3137 | Entropy: 1.8813\n",
      "Mean reward this rollout: 0.1348\n",
      "Mean reward this rollout: 0.5950\n",
      "Mean reward this rollout: 1.0867\n",
      "Mean reward this rollout: 1.0598\n",
      "Mean reward this rollout: 0.6215\n",
      "Mean reward this rollout: 0.6645\n",
      "Mean reward this rollout: 0.2571\n",
      "Mean reward this rollout: 0.3955\n",
      "Mean reward this rollout: 0.8552\n",
      "Mean reward this rollout: 0.1229\n",
      "Update 220/976 | Timesteps: 225280/1000000\n",
      "  Actor Loss: -0.0446 | Critic Loss: 7.4107 | Entropy: 1.8538\n",
      "Mean reward this rollout: 0.5069\n",
      "Mean reward this rollout: 0.7537\n",
      "Mean reward this rollout: 1.0059\n",
      "Mean reward this rollout: 0.6140\n",
      "Mean reward this rollout: 0.9959\n",
      "Mean reward this rollout: 1.1921\n",
      "Mean reward this rollout: 1.3402\n",
      "Mean reward this rollout: 0.4649\n",
      "Mean reward this rollout: 0.7024\n",
      "Mean reward this rollout: 0.6100\n",
      "Update 230/976 | Timesteps: 235520/1000000\n",
      "  Actor Loss: -0.0353 | Critic Loss: 8.5942 | Entropy: 2.1404\n",
      "Mean reward this rollout: 0.7955\n",
      "Mean reward this rollout: 0.6494\n",
      "Mean reward this rollout: 1.3901\n",
      "Mean reward this rollout: 1.4309\n",
      "Mean reward this rollout: 0.2734\n",
      "Mean reward this rollout: 1.1777\n",
      "Mean reward this rollout: 1.9803\n",
      "Mean reward this rollout: 0.3762\n",
      "Mean reward this rollout: 0.3206\n",
      "Mean reward this rollout: 0.5903\n",
      "Update 240/976 | Timesteps: 245760/1000000\n",
      "  Actor Loss: -0.0480 | Critic Loss: 52.3544 | Entropy: 2.0465\n",
      "Mean reward this rollout: 0.4122\n",
      "Mean reward this rollout: 0.6443\n",
      "Mean reward this rollout: 0.9955\n",
      "Mean reward this rollout: 1.3451\n",
      "Mean reward this rollout: 0.9693\n",
      "Mean reward this rollout: 0.7492\n",
      "Mean reward this rollout: 0.5692\n",
      "Mean reward this rollout: 0.7128\n",
      "Mean reward this rollout: 1.6732\n",
      "Mean reward this rollout: 1.8924\n",
      "Update 250/976 | Timesteps: 256000/1000000\n",
      "  Actor Loss: 0.0071 | Critic Loss: 194.7924 | Entropy: 2.2231\n",
      "Mean reward this rollout: 1.0052\n",
      "Mean reward this rollout: 0.7045\n",
      "Mean reward this rollout: 0.6541\n",
      "Mean reward this rollout: 0.7991\n",
      "Mean reward this rollout: 1.0851\n",
      "Mean reward this rollout: 0.8555\n",
      "Mean reward this rollout: 1.1853\n",
      "Mean reward this rollout: 0.6658\n",
      "Mean reward this rollout: 1.0582\n",
      "Mean reward this rollout: 1.8249\n",
      "Update 260/976 | Timesteps: 266240/1000000\n",
      "  Actor Loss: -0.0291 | Critic Loss: 197.2320 | Entropy: 2.5256\n",
      "Mean reward this rollout: 1.1986\n",
      "Mean reward this rollout: 1.2676\n",
      "Mean reward this rollout: 2.2231\n",
      "Mean reward this rollout: 1.6939\n",
      "Mean reward this rollout: 1.0174\n",
      "Mean reward this rollout: 1.0299\n",
      "Mean reward this rollout: 1.5982\n",
      "Mean reward this rollout: 1.4849\n",
      "Mean reward this rollout: 1.4597\n",
      "Mean reward this rollout: 1.5371\n",
      "Update 270/976 | Timesteps: 276480/1000000\n",
      "  Actor Loss: -0.0583 | Critic Loss: 55.0051 | Entropy: 2.4016\n",
      "Mean reward this rollout: 1.0159\n",
      "Mean reward this rollout: 0.9551\n",
      "Mean reward this rollout: 1.9416\n",
      "Mean reward this rollout: 1.2057\n",
      "Mean reward this rollout: 2.1663\n",
      "Mean reward this rollout: 1.0201\n",
      "Mean reward this rollout: 1.2078\n",
      "Mean reward this rollout: 0.5651\n",
      "Mean reward this rollout: 1.3357\n",
      "Mean reward this rollout: 0.7629\n",
      "Update 280/976 | Timesteps: 286720/1000000\n",
      "  Actor Loss: -0.0697 | Critic Loss: 819.8521 | Entropy: 2.3363\n",
      "Mean reward this rollout: 0.8408\n",
      "Mean reward this rollout: 0.9461\n",
      "Mean reward this rollout: 0.9065\n",
      "Mean reward this rollout: 0.6159\n",
      "Mean reward this rollout: 1.7299\n",
      "Mean reward this rollout: 1.3425\n",
      "Mean reward this rollout: 1.0600\n",
      "Mean reward this rollout: 0.9621\n",
      "Mean reward this rollout: 2.9465\n",
      "Mean reward this rollout: 1.9510\n",
      "Update 290/976 | Timesteps: 296960/1000000\n",
      "  Actor Loss: -0.0720 | Critic Loss: 80.5201 | Entropy: 2.4906\n",
      "Mean reward this rollout: 1.6100\n",
      "Mean reward this rollout: 1.6589\n",
      "Mean reward this rollout: 0.7669\n",
      "Mean reward this rollout: 1.2257\n",
      "Mean reward this rollout: 0.8634\n",
      "Mean reward this rollout: 2.3188\n",
      "Mean reward this rollout: 1.2048\n",
      "Mean reward this rollout: 1.6111\n",
      "Mean reward this rollout: 1.7450\n",
      "Mean reward this rollout: 1.4075\n",
      "Update 300/976 | Timesteps: 307200/1000000\n",
      "  Actor Loss: -0.0455 | Critic Loss: 514.9840 | Entropy: 2.4871\n",
      "Saving checkpoint at update 300...\n",
      "MAPPO models saved with prefix models/mappo_update300\n",
      "Mean reward this rollout: 1.3550\n",
      "Mean reward this rollout: 1.8834\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "vec_env = VectorizedEnv(\n",
    "    Environment, num_envs=NUM_ENVS,\n",
    "    map_file=MAP_FILE,\n",
    "    n_robots=NUM_AGENTS,\n",
    "    n_packages=N_PACKAGES,\n",
    "    move_cost=MOVE_COST,\n",
    "    delivery_reward=DELIVERY_REWARD,\n",
    "    delay_reward=DELAY_REWARD,\n",
    "    seed=SEED, # Seed for each sub-environment will be SEED, SEED+1, ...\n",
    "    max_time_steps=MAX_TIME_STEPS_PER_EPISODE\n",
    ")\n",
    "\n",
    "# Determine observation and global state shapes from one env instance\n",
    "_temp_env = Environment(map_file=MAP_FILE, n_robots=NUM_AGENTS, n_packages=N_PACKAGES, move_cost=MOVE_COST, delivery_reward=DELIVERY_REWARD, delay_reward=DELAY_REWARD, seed=SEED, max_time_steps=MAX_TIME_STEPS_PER_EPISODE)\n",
    "\n",
    "        \n",
    "OBS_SHAPE = (6, _temp_env.n_rows, _temp_env.n_cols)\n",
    "GLOBAL_STATE_SHAPE = (7, _temp_env.n_rows, _temp_env.n_cols)\n",
    "\n",
    "print(f\"Obs shape: {OBS_SHAPE}, Global state shape: {GLOBAL_STATE_SHAPE}\")\n",
    "\n",
    "trainer = MAPPOTrainer(vec_env, NUM_AGENTS, ACTION_DIM, OBS_SHAPE, GLOBAL_STATE_SHAPE)\n",
    "\n",
    "# Load existing model if available\n",
    "# load_mappo_model(trainer.actor, trainer.critic) # Uncomment to load\n",
    "\n",
    "episode_rewards_history = []\n",
    "actor_loss_history = []\n",
    "critic_loss_history = []\n",
    "entropy_history = []\n",
    "\n",
    "print(\"Starting MAPPO training...\")\n",
    "\n",
    "# Initial reset and state preparation\n",
    "current_env_states_list = vec_env.reset() # List of state dicts\n",
    "current_local_obs_list = torch.zeros((NUM_ENVS, NUM_AGENTS, *OBS_SHAPE), device=\"cpu\")\n",
    "current_global_states_list = torch.zeros((NUM_ENVS, *GLOBAL_STATE_SHAPE), device=\"cpu\")\n",
    "\n",
    "for env_idx in range(NUM_ENVS):\n",
    "    \n",
    "    trainer._update_persistent_packages_for_env(env_idx, current_env_states_list[env_idx])\n",
    "    current_persistent_packages = trainer.persistent_packages_list[env_idx]\n",
    "    current_global_states_list[env_idx] = torch.from_numpy(convert_state(\n",
    "                                                        current_env_states_list[env_idx], \n",
    "                                                        current_persistent_packages, \n",
    "                                                        GLOBAL_STATE_SHAPE\n",
    "                                                        ))\n",
    "    for agent_idx in range(NUM_AGENTS):\n",
    "        current_local_obs_list[env_idx, agent_idx] = torch.from_numpy(\n",
    "            convert_observation(current_env_states_list[env_idx], current_persistent_packages, agent_idx)\n",
    "        ).float()\n",
    "\n",
    "num_updates = TOTAL_TIMESTEPS // (ROLLOUT_STEPS * NUM_ENVS)\n",
    "total_steps_done = 0\n",
    "\n",
    "try:\n",
    "    for update_num in range(1, num_updates + 1):\n",
    "        \n",
    "        (b_obs, b_global_states, b_actions, b_log_probs_old, b_advantages, b_returns_critic,\n",
    "        current_env_states_list, current_local_obs_list, current_global_states_list,mb_rewards\n",
    "        ) = trainer.collect_rollouts(current_env_states_list, current_local_obs_list, current_global_states_list)\n",
    "\n",
    "\n",
    "        actor_loss, critic_loss, entropy = trainer.update_ppo(\n",
    "            b_obs, b_global_states, b_actions, b_log_probs_old, b_advantages, b_returns_critic\n",
    "        )\n",
    "        \n",
    "        total_steps_done += ROLLOUT_STEPS * NUM_ENVS\n",
    "        \n",
    "        # For logging, we might need to track rewards from rollouts\n",
    "        # Print mean reward for this rollout\n",
    "        mean_reward = mb_rewards.mean().item()\n",
    "        print(f\"Mean reward this rollout: {mean_reward:.4f}\")\n",
    "        episode_rewards_history.append(mean_reward)\n",
    "\n",
    "        actor_loss_history.append(actor_loss)\n",
    "        critic_loss_history.append(critic_loss)\n",
    "        entropy_history.append(entropy)\n",
    "\n",
    "        if update_num % 10 == 0: # Log every 10 updates\n",
    "            print(f\"Update {update_num}/{num_updates} | Timesteps: {total_steps_done}/{TOTAL_TIMESTEPS}\")\n",
    "            print(f\"  Actor Loss: {actor_loss:.4f} | Critic Loss: {critic_loss:.4f} | Entropy: {entropy:.4f}\")\n",
    "        \n",
    "        if update_num % 100 == 0: # Save model periodically\n",
    "            print(f\"Saving checkpoint at update {update_num}...\")\n",
    "            save_mappo_model(trainer.actor, trainer.critic, path_prefix=f\"models/mappo_update{update_num}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    print(\"Saving final model...\")\n",
    "    pygame.quit()\n",
    "    save_mappo_model(trainer.actor, trainer.critic, path_prefix=\"models/mappo_final\")\n",
    "    print(\"\\nTraining loop finished or was interrupted.\")\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(24, 5))\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.plot(episode_rewards_history)\n",
    "    plt.title('Mean Reward per Rollout')\n",
    "    plt.xlabel('Update Number')\n",
    "    plt.ylabel('Mean Reward')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.plot(actor_loss_history)\n",
    "    plt.title('Actor Loss per Update')\n",
    "    plt.xlabel('Update Number')\n",
    "    plt.ylabel('Actor Loss')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.plot(critic_loss_history)\n",
    "    plt.title('Critic Loss per Update')\n",
    "    plt.xlabel('Update Number')\n",
    "    plt.ylabel('Critic Loss')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.plot(entropy_history)\n",
    "    plt.title('Policy Entropy per Update')\n",
    "    plt.xlabel('Update Number')\n",
    "    plt.ylabel('Entropy')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "rl-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
